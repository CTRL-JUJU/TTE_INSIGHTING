{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading & Setup\n",
    "This section loads the dataset and initializes necessary directories.  \n",
    "The dataset contains information on patients, their treatment status, and survival outcomes.  \n",
    "To improve clustering performance, we apply **StandardScaler** to normalize numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block 1: Load dataset and preprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Create necessary directories\n",
    "trial_pp_dir = \"trial_pp\"\n",
    "trial_itt_dir = \"trial_itt\"\n",
    "os.makedirs(trial_pp_dir, exist_ok=True)\n",
    "os.makedirs(trial_itt_dir, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "data_censored = pd.read_csv(\"data_censored.csv\")\n",
    "\n",
    "# Standardize features for clustering\n",
    "scaler = StandardScaler()\n",
    "features = data_censored[[\"age\", \"x1\", \"x2\", \"x3\"]]\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clustering Implementation\n",
    "To identify patterns within the dataset, we apply clustering.  \n",
    "The options include **DBSCAN, Agglomerative Clustering, and Gaussian Mixture Model (GMM)**.  \n",
    "The chosen method segments patients into groups, allowing for better survival analysis insights.  \n",
    "Clusters are then added to the dataset for use in later modeling steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block 2: Apply Clustering\n",
    "def apply_clustering(data, method='DBSCAN'):\n",
    "    if method == 'DBSCAN':\n",
    "        cluster_model = DBSCAN(eps=0.5, min_samples=5)\n",
    "    elif method == 'Agglomerative':\n",
    "        cluster_model = AgglomerativeClustering(n_clusters=3)\n",
    "    elif method == 'GMM':\n",
    "        cluster_model = GaussianMixture(n_components=3, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid clustering method\")\n",
    "    \n",
    "    labels = cluster_model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "# Apply clustering and assign cluster labels\n",
    "data_censored['Cluster'] = apply_clustering(scaled_features, method='DBSCAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Treatment Switching & Censoring Weight Adjustments per Cluster\n",
    "For each identified cluster, a **logistic regression model** predicts the likelihood of treatment switching.  \n",
    "This ensures weights are assigned appropriately based on different patient subgroups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'period', 'treatment', 'x1', 'x2', 'x3', 'x4', 'age', 'age_s',\n",
      "       'outcome', 'censored', 'eligible', 'Cluster', 'switch_weight',\n",
      "       'censor_weight', 'final_weight'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Code Block 3: Logistic Regression for Treatment Switching\n",
    "def logistic_regression_per_cluster(df, features, target):\n",
    "    weights = []\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        subset = df[df['Cluster'] == cluster]\n",
    "        if len(subset) > 10:  # Ensure sufficient data points\n",
    "            model = LogisticRegression()\n",
    "            model.fit(subset[features], subset[target])\n",
    "            weights.append(model.predict_proba(subset[features])[:, 1])\n",
    "        else:\n",
    "            weights.append(np.ones(len(subset)))\n",
    "    return np.concatenate(weights)\n",
    "\n",
    "# Compute weights\n",
    "data_censored[\"switch_weight\"] = logistic_regression_per_cluster(data_censored, [\"age\", \"x1\", \"x3\"], \"treatment\")\n",
    "data_censored[\"censor_weight\"] = logistic_regression_per_cluster(data_censored, [\"x2\", \"x1\"], \"censored\")\n",
    "data_censored[\"final_weight\"] = data_censored[\"switch_weight\"] * data_censored[\"censor_weight\"]\n",
    "\n",
    "print(data_censored.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Outcome Model Using Logistic Regression\n",
    "A logistic regression model estimates the treatment effect on survival probability.  \n",
    "This helps analyze how different factors contribute to the likelihood of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'period', 'treatment', 'x1', 'x2', 'x3', 'x4', 'age', 'age_s',\n",
      "       'outcome', 'censored', 'eligible', 'Cluster', 'switch_weight',\n",
      "       'censor_weight', 'final_weight'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['assigned_treatment', 'followup_time'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Code Block 4: Outcome Model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_censored\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m----> 5\u001b[0m X_outcome \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39madd_constant(data_censored[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massigned_treatment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowup_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m      6\u001b[0m y_outcome \u001b[38;5;241m=\u001b[39m data_censored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcome\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m outcome_model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mLogit(y_outcome, X_outcome)\u001b[38;5;241m.\u001b[39mfit()\n",
      "File \u001b[1;32mc:\\Users\\Niles Rondez\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Niles Rondez\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Niles Rondez\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['assigned_treatment', 'followup_time'] not in index\""
     ]
    }
   ],
   "source": [
    "# Ensure necessary columns exist\n",
    "if \"assigned_treatment\" not in data_censored.columns:\n",
    "    data_censored[\"assigned_treatment\"] = data_censored[\"treatment\"]  \n",
    "if \"followup_time\" not in data_censored.columns:\n",
    "    data_censored[\"followup_time\"] = data_censored[\"period\"]\n",
    "\n",
    "# Outcome Model\n",
    "X_outcome = sm.add_constant(data_censored[[\"assigned_treatment\", \"x2\", \"followup_time\"]])\n",
    "y_outcome = data_censored[\"outcome\"]\n",
    "outcome_model = sm.Logit(y_outcome, X_outcome).fit()\n",
    "print(outcome_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Expanding Trial Periods\n",
    "To simulate real-world patient follow-up, we expand trial periods.  \n",
    "Each period represents a step in patient monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block 5: Expanding Trial Periods\n",
    "def expand_trials(data):\n",
    "    expanded_data = data.copy()\n",
    "    expanded_data[\"trial_period\"] = expanded_data[\"period\"]\n",
    "    expanded_data[\"followup_time\"] = expanded_data[\"period\"] + 1\n",
    "    return expanded_data\n",
    "\n",
    "trial_pp_expanded = expand_trials(data_censored)\n",
    "trial_itt_expanded = expand_trials(data_censored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating Sequential Trials\n",
    "Patients are tracked for up to five future periods, creating a sequence of follow-ups.  \n",
    "This provides richer insights into long-term survival trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block 6: Creating Sequential Trials\n",
    "def create_sequence_of_trials(data):\n",
    "    expanded_trials = []\n",
    "    for _, row in data.iterrows():\n",
    "        period_start = int(row[\"period\"])\n",
    "        period_end = period_start + 5\n",
    "        for t in range(period_start, period_end):\n",
    "            new_row = row.copy()\n",
    "            new_row[\"trial_period\"] = t\n",
    "            new_row[\"followup_time\"] = t - period_start\n",
    "            expanded_trials.append(new_row)\n",
    "    return pd.DataFrame(expanded_trials)\n",
    "\n",
    "trial_pp_seq = create_sequence_of_trials(trial_pp_expanded)\n",
    "trial_itt_seq = create_sequence_of_trials(trial_itt_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Weight Adjustments\n",
    "To prevent extreme weighting, we apply **Winsorization** to cap the highest values at the 99th percentile.  \n",
    "This stabilizes model performance and ensures fair weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block 7: Weight Adjustments\n",
    "def winsorize_weights(weights):\n",
    "    q99 = np.percentile(weights, 99)\n",
    "    return np.minimum(weights, q99)\n",
    "\n",
    "sample_size = int(len(trial_itt_expanded) * 0.5)\n",
    "trial_itt_sampled = trial_itt_expanded.sample(sample_size, random_state=1234)\n",
    "trial_itt_sampled[\"adjusted_weight\"] = winsorize_weights(trial_itt_sampled[\"final_weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. MSM Model Fitting\n",
    "A logistic regression model is trained using **Inverse Probability Weighting (IPW)**  \n",
    "to estimate treatment effects on survival probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block 8: MSM Model Fitting\n",
    "X_msm = sm.add_constant(trial_itt_sampled[[\"assigned_treatment\", \"x2\", \"followup_time\"]])\n",
    "y_msm = trial_itt_sampled[\"outcome\"]\n",
    "\n",
    "msm_model = sm.Logit(y_msm, X_msm, weights=trial_itt_sampled[\"adjusted_weight\"]).fit()\n",
    "print(msm_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Kaplan-Meier Survival Analysis by Cluster\n",
    "The **Kaplan-Meier estimator** is used to plot survival curves for each cluster.  \n",
    "This allows for a direct comparison of how different clusters behave over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block 9: Kaplan-Meier Survival Analysis by Cluster\n",
    "def survival_analysis(df):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    has_clusters = False\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        subset = df[df['Cluster'] == cluster]\n",
    "        if len(subset) > 5:\n",
    "            kmf.fit(subset['followup_time'], event_observed=subset['outcome'])\n",
    "            kmf.plot(label=f'Cluster {cluster}')\n",
    "            has_clusters = True\n",
    "    \n",
    "    plt.title(\"Kaplan-Meier Survival Curves by Cluster\")\n",
    "    plt.xlabel(\"Follow-up Time\")\n",
    "    plt.ylabel(\"Survival Probability\")\n",
    "    if has_clusters:\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "survival_analysis(data_censored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Cluster Distribution Table\n",
    "This table displays the number of data points assigned to each cluster.  \n",
    "It provides insights into how patient groups are distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block 10: Cluster Distribution Table\n",
    "print(\"Cluster Distribution Table:\")\n",
    "print(data_censored.groupby(\"Cluster\").size())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
